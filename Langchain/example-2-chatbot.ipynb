{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7faa80",
   "metadata": {},
   "source": [
    "##Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60079982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import trim_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373f02c",
   "metadata": {},
   "source": [
    "### Modellinitialisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b1e4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/pxcb698d1zq7fylq212mnvch0000gn/T/ipykernel_10430/1457499772.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"llama3.1\")\n"
     ]
    }
   ],
   "source": [
    "model = ChatOllama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02cf017",
   "metadata": {},
   "source": [
    "### Stateless Chat Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b74186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Bob! What brings you here today? Do you have a question, need some assistance or just want to chat? I'm all ears!\n",
      "I'm happy to chat with you, but I don't actually know your name. This is our first conversation, and I don't have any information about you. Would you like to introduce yourself?\n"
     ]
    }
   ],
   "source": [
    "stateless_response = model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n",
    "print(stateless_response.content)\n",
    "\n",
    "followup_response = model.invoke([HumanMessage(content=\"What's my name?\")])\n",
    "print(followup_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abcb36",
   "metadata": {},
   "source": [
    "### Conversation Memory mit Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f27ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f02a3",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a25daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9035b",
   "metadata": {},
   "source": [
    "### Kombination: Komplexe Chain + Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dab9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794087a9",
   "metadata": {},
   "source": [
    "### Trimmen der History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=40,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "chain_with_trimmer = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

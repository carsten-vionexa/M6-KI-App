from pathlib import Path

readme_content = """# ğŸ“˜ RAG-Hawkins â€“ Seminar-RAG-System

## ğŸ” Projektbeschreibung
Dieses Projekt implementiert ein eigenes **RAG-System (Retrieval-Augmented Generation)** fÃ¼r *Hawkins Consulting*.
Ziel: Fragen zu Seminaren und Trainings auf Basis interner PDF-Dokumente (z.â€¯B. Prospekte, ThemenÃ¼bersichten) automatisch beantworten.

Das System kombiniert:
- **Chroma Vectorstore** fÃ¼r Dokument-Suche
- **OpenAI GPT-4o-mini** als LLM fÃ¼r Antwortgenerierung
- **LangChain Memory** fÃ¼r Konversationskontext
- **Streamlit** als UI

---

## âš™ï¸ Projektstruktur

```
RAG-Hawkins/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ init_db.py
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ chat_chain.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ qa_chain.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ semantics.py
â”‚   â””â”€â”€ vectorstore.py
â”‚
â”œâ”€â”€ pdf/
â”‚   â””â”€â”€ (Seminarunterlagen & Prospekte)
â”‚
â””â”€â”€ chroma_db/
    â””â”€â”€ (persistenter Vectorstore)
```

---

## ğŸ§© DateiÃ¼bersicht

### ğŸ–¥ï¸ app.py
Streamlit-Frontend.  
- Steuert Benutzerinteraktion und Darstellung.  
- Nimmt Benutzerfragen entgegen.  
- Ruft relevante Dokumente aus der Chroma-DB ab.  
- Leitet alles an das LLM mit Conversation Memory weiter.  
- Zeigt Antwort, Verlauf und Quellen an.

### ğŸ§± init_db.py
Einmaliger Initialisierer der Vektordatenbank.  
- LÃ¶scht alte Chroma-DB (sauberer Neuaufbau).  
- LÃ¤dt PDF-Dateien aus `/pdf`.  
- Teilt sie in Chunks.  
- Erstellt Embeddings (OpenAI).  
- Speichert alles persistent in `chroma_db/`.

### ğŸ§  rag/embedder.py
Definiert das Embedding-Modell.  
- Standard: `OpenAIEmbeddings(model="text-embedding-3-large")`.  
- Dient als Schnittstelle zwischen Text und Vektorraum.

### ğŸ“„ rag/loader.py
LÃ¤dt Dokumente.  
- Liest PDF-Dateien aus einem Verzeichnis.  
- Nutzt `PyPDFLoader` aus `langchain_community`.  
- Ãœbergibt Textsegmente an den Splitter.

### âœ‚ï¸ rag/semantics.py
Verantwortlich fÃ¼r Textaufteilung.  
- EnthÃ¤lt verschiedene Split-Methoden (z.â€¯B. semantisch oder klassisch).  
- Steuert ChunkgrÃ¶ÃŸe und Ãœberlappung.  
- Bereitet Daten fÃ¼r das Einbetten in die Chroma-DB vor.

### ğŸ§® rag/vectorstore.py
Schnittstelle zur Chroma-Datenbank.  
- Erstellt oder lÃ¤dt persistente Datenbank.  
- Speichert Dokumente und ihre Embeddings.  
- Stellt `create_or_load_chroma()` fÃ¼r `init_db.py` bereit.

### ğŸ” rag/retriever.py
FÃ¼hrt semantische Suche in der Chroma-DB aus.  
- Findet relevante Dokumentsegmente zu einer Benutzerfrage.  
- RÃ¼ckgabe: Liste von LangChain-Dokumentobjekten mit `page_content` & `metadata`.

### ğŸ’¬ rag/chat_chain.py
Implementiert den **Konversationsspeicher** mit `RunnableWithMessageHistory`.  
- Baut GPT-4o-Chain mit Chat-Memory auf.  
- Jede Session behÃ¤lt ihren GesprÃ¤chsverlauf.  
- Wird in `app.py` als Session-Objekt verwaltet.

### ğŸ¯ rag/qa_chain.py
Steuert die **Frage-Antwort-Logik**.  
- Kombiniert Frage + Kontext (Retriever-Ergebnisse).  
- Ãœbergibt Prompt an das LLM.  
- Extrahiert und gibt nur den Antworttext zurÃ¼ck.

---

## ğŸ§  Datenfluss (Kurz erklÃ¤rt)

1. **init_db.py**  
   â†’ PDF laden â†’ Chunks erstellen â†’ Embeddings erzeugen â†’ Chroma speichern

2. **app.py**  
   â†’ Userfrage â†’ Retriever (Chroma) â†’ relevante Textstellen  
   â†’ ChatChain (mit Memory) â†’ Antwort generieren  
   â†’ Ausgabe: Antwort + Quellen + Verlauf

---

## ğŸš€ Start

```bash
# 1. Chroma-DB neu aufbauen
python init_db.py

# 2. App starten
streamlit run app.py
```

---

## âš¡ Technologie-Stack
- **Python 3.11+**
- **LangChain 0.3.x**
- **Chroma 0.4.x**
- **OpenAI GPT-4o / GPT-4o-mini**
- **Streamlit 1.40+**
- **PyPDF / BeautifulSoup / Requests**
"""
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f61c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086d79c",
   "metadata": {},
   "source": [
    "### Datenquellen laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a08651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Dokument(e) geladen\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, e\n"
     ]
    }
   ],
   "source": [
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "print(len(docs), \"Dokument(e) geladen\")\n",
    "print(docs[0].page_content[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3098aa2",
   "metadata": {},
   "source": [
    "### Texte in Chunks zerlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7bb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 Chunks erzeugt\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"{len(splits)} Chunks erzeugt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f169910",
   "metadata": {},
   "source": [
    "### Embedding in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd109bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"all-minilm:33m\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"lilian_agents\",\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c960aa",
   "metadata": {},
   "source": [
    "### Test des Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f212f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Treffer 1 ---\n",
      "\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-tim\n",
      "\n",
      "--- Treffer 2 ---\n",
      "\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a cl\n",
      "\n",
      "--- Treffer 3 ---\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the approaches to Task Decomposition?\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "for i, doc in enumerate(results[:3], 1):\n",
    "    print(f\"\\n--- Treffer {i} ---\\n\")\n",
    "    print(doc.page_content[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936760b",
   "metadata": {},
   "source": [
    "### LLM und Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cbc4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1:latest\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a concise assistant. \n",
    "Use only the provided context to answer the question.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e2cf2",
   "metadata": {},
   "source": [
    "### RAG-Chain bauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a12e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16206ed",
   "metadata": {},
   "source": [
    "### Frage stellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e487175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition in LLM agents refers to breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. According to the provided context, task decomposition is achieved through techniques such as Chain of Thought (CoT), which instructs the model to \"think step by step\" and decompose hard tasks into simpler steps."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is task decomposition in LLM agents?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

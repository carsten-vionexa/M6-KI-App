{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Daily Research Monitor ‚Äì Hugging Face Top Papers\n",
    "Startdatum: 29.10.2025\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import datetime\n",
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "from huggingface_hub import HfApi\n",
    "import arxiv\n",
    "from pypdf import PdfReader\n",
    "from smolagents import tool, CodeAgent, LiteLLMModel\n",
    "\n",
    "# ==============================\n",
    "# Datenbank-Funktionen\n",
    "# ==============================\n",
    "\n",
    "DB_FILE = \"papers.db\"\n",
    "\n",
    "def init_db():\n",
    "    \"\"\"Erstellt Tabelle, falls sie noch nicht existiert.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            authors TEXT,\n",
    "            summary TEXT,\n",
    "            date_processed TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def already_processed(paper_id):\n",
    "    \"\"\"Pr√ºft, ob Paper-ID bereits in DB existiert.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT 1 FROM papers WHERE id=?\", (paper_id,))\n",
    "    found = cur.fetchone()\n",
    "    conn.close()\n",
    "    return found is not None\n",
    "\n",
    "def add_paper(paper):\n",
    "    \"\"\"Speichert neues Paper in DB.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Autorenfeld in String umwandeln, falls Liste o.√§.\n",
    "    authors = paper.get(\"authors\", \"\")\n",
    "    if isinstance(authors, (list, tuple)):\n",
    "        authors = \", \".join(authors)\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT OR IGNORE INTO papers (id, title, authors, summary, date_processed)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        str(paper.get(\"id\", \"\")),\n",
    "        str(paper.get(\"title\", \"\")),\n",
    "        authors,\n",
    "        str(paper.get(\"summary\", \"\")),\n",
    "        datetime.date(2025, 10, 29).isoformat()\n",
    "    ))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# ==============================\n",
    "# Tools\n",
    "# ==============================\n",
    "\n",
    "@tool\n",
    "def get_top_three_papers() -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of the top 3 daily papers (titles) from Hugging Face.\n",
    "    \"\"\"\n",
    "    url = \"https://huggingface.co/papers\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    containers = soup.find_all('div', class_='SVELTE_HYDRATER contents')\n",
    "    titles = []\n",
    "\n",
    "    for container in containers:\n",
    "        data_props = container.get('data-props', '')\n",
    "        if not data_props:\n",
    "            continue\n",
    "        try:\n",
    "            data_json = json.loads(data_props.replace('&quot;', '\"'))\n",
    "            if 'dailyPapers' in data_json:\n",
    "                for paper in data_json['dailyPapers'][:3]:\n",
    "                    titles.append(paper['title'])\n",
    "                break\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    return titles or [\"No papers found.\"]\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_paper_id_by_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the arXiv paper ID by its title.\n",
    "\n",
    "    Args:\n",
    "        title: The paper title for which to get the ID.\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    papers = api.list_papers(query=title)\n",
    "    if papers:\n",
    "        paper = next(iter(papers))\n",
    "        return paper.id\n",
    "    else:\n",
    "        return \"No paper ID found.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def download_paper_by_id(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads the arXiv paper by ID and saves it as 'paper_<id>.pdf'.\n",
    "\n",
    "    Args:\n",
    "        paper_id: The arXiv ID of the paper to download (e.g., \"1706.03762\").\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        paper = next(client.results(search))\n",
    "        filename = f\"paper_{paper_id}.pdf\"\n",
    "        paper.download_pdf(filename=filename)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {e}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the first 3 pages of a PDF and returns text.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages[:3]:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading {file_path}: {e}\"\n",
    "\n",
    "# ==============================\n",
    "# Hauptlogik\n",
    "# ==============================\n",
    "\n",
    "def main():\n",
    "    init_db()\n",
    "    model = LiteLLMModel(model_id=\"ollama_chat/glm-4.6:cloud\")\n",
    "\n",
    "    agent = CodeAgent(\n",
    "        tools=[get_top_three_papers, get_paper_id_by_title, download_paper_by_id, read_pdf_file],\n",
    "        model=model,\n",
    "        stream_outputs=False\n",
    "    )\n",
    "\n",
    "    result = agent.run(\"\"\"\n",
    "    Fetch the top 3 papers from Hugging Face daily papers.\n",
    "    For each paper:\n",
    "    - get its arXiv ID,\n",
    "    - download it,\n",
    "    - read the first pages,\n",
    "    - extract author list if available,\n",
    "    - and summarize it.\n",
    "    Return structured JSON with: title, id, authors, summary.\n",
    "    \"\"\")\n",
    "\n",
    "    new_papers = []\n",
    "\n",
    "    # ü©π Abfanglogik f√ºr unterschiedliche Typen\n",
    "    if hasattr(result, \"text\"):           # AgentText-Objekt ‚Üí Text extrahieren\n",
    "        import json\n",
    "        try:\n",
    "            parsed = json.loads(result.text)\n",
    "            papers = parsed if isinstance(parsed, list) else parsed.get(\"papers\", [])\n",
    "        except Exception:\n",
    "            print(\"‚ö†Ô∏è  Output war kein valides JSON. Keine neuen Paper gefunden.\")\n",
    "            papers = []\n",
    "    elif isinstance(result, list):\n",
    "        papers = result\n",
    "    elif isinstance(result, dict):\n",
    "        papers = result.get(\"papers\", [])\n",
    "    else:\n",
    "        papers = []\n",
    "\n",
    "    for paper in papers:\n",
    "        if not already_processed(paper[\"id\"]):\n",
    "            add_paper(paper)\n",
    "            new_papers.append(paper)\n",
    "\n",
    "    if not new_papers:\n",
    "        print(\"No new papers found.\")\n",
    "    else:\n",
    "        print(\"\\n=== NEW PAPERS ===\")\n",
    "        for p in new_papers:\n",
    "            print(f\"- {p['title']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

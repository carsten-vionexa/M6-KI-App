{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e9d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/trans/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n",
      "  Code Intelligence\n"
     ]
    }
   ],
   "source": [
    "from smolagents import tool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def get_hugging_face_top_daily_paper() -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the most upvoted paper on Hugging Face daily papers.\n",
    "    It returns the title of the paper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://huggingface.co/papers\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        containers = soup.find_all('div', class_='SVELTE_HYDRATER contents')\n",
    "        top_paper = \"\"\n",
    "\n",
    "        for container in containers:\n",
    "            data_props = container.get('data-props', '')\n",
    "            if data_props:\n",
    "                try:\n",
    "                    json_data = json.loads(data_props.replace('&quot;', '\"'))\n",
    "                    if 'dailyPapers' in json_data:\n",
    "                        top_paper = json_data['dailyPapers'][0]['title']\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return top_paper or \"No paper found.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching papers: {e}\"\n",
    "\n",
    "# ğŸ‘‰ Testlauf\n",
    "print(get_hugging_face_top_daily_paper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35bab462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510.23538\n"
     ]
    }
   ],
   "source": [
    "from smolagents import tool\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "@tool\n",
    "def get_paper_id_by_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool returns the arXiv paper ID by its title.\n",
    "    \n",
    "    Args:\n",
    "        title: The paper title for which to get the ID.\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    papers = api.list_papers(query=title)\n",
    "    if papers:\n",
    "        paper = next(iter(papers))\n",
    "        return paper.id\n",
    "    else:\n",
    "        return \"No paper ID found.\"\n",
    "\n",
    "# ğŸ‘‰ Testlauf mit deinem Titel\n",
    "print(get_paper_id_by_title(\"JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803adb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence as paper.pdf\n"
     ]
    }
   ],
   "source": [
    "from smolagents import tool\n",
    "import arxiv\n",
    "\n",
    "@tool\n",
    "def download_paper_by_id(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool gets the ID of a paper and downloads it from arXiv.\n",
    "    It saves the paper locally as 'paper.pdf'.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The arXiv ID of the paper to download (e.g., \"1706.03762\").\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        paper = next(client.results(search))\n",
    "        paper.download_pdf(filename=\"paper.pdf\")\n",
    "        return f\"Downloaded {paper.title} as paper.pdf\"\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {e}\"\n",
    "\n",
    "# ğŸ‘‰ Testlauf\n",
    "print(download_paper_by_id(\"2510.23538\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b3e016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprint\n",
      "JANUSCODER: TOWARDS AFOUNDATIONALVISUAL-\n",
      "PROGRAMMATICINTERFACE FORCODEINTELLIGENCE\n",
      "Qiushi Sunâ™¡â™¢âˆ—\n",
      "Jingyang Gongâ™¡* Yang Liu* Qiaosheng Chen* Lei Liâ‹† Kai Chenâ™¢\n",
      "Qipeng Guoâ™¢á¼´BBen Kaoâ™¡ Fei Yuanâ™¢\n",
      "â™¡The University of Hong Kong â™¢Shanghai AI Laboratory Nanjing University\n",
      "â‹†Carnegie Mellon University á¼´BShanghai Innovation Institute\n",
      "qiushisun@connect.hku.hk,jingyang.gong@nyu.edu\n",
      "qschen@smail.nju.edu.cn,yliu20.nju@gmail.com,leili@cs.cmu.edu\n",
      "kao@cs.hku.hk,{chenkai,guoqipeng,yuanfei}@pjlab.org.cn\n",
      "ABSTRACT\n",
      "The scope of neural code intelligence is rapidly expanding beyond text-based source\n",
      "code to encompass the rich visual outputs that programs generate. This visual\n",
      "dimension is critical for advanced applications like flexible content generation and\n",
      "precise, program-driven editing of visualizations. However, progress has been im-\n",
      "peded by the scarcity of high-quality multi-modal code data, a bottleneck stemming\n",
      "from challenges in synthesis and quality assessment. To address these challenges,\n",
      "we \n"
     ]
    }
   ],
   "source": [
    "from smolagents import tool\n",
    "from pypdf import PdfReader\n",
    "\n",
    "@tool\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the first three pages of a PDF file and returns its content as text.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        pages = reader.pages[:3]\n",
    "        content = \"\"\n",
    "        for page in pages:\n",
    "            content += page.extract_text() or \"\"\n",
    "        return content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {e}\"\n",
    "\n",
    "# ğŸ‘‰ Testlauf\n",
    "text = read_pdf_file(\"paper.pdf\")\n",
    "print(text[:1000])  # nur die ersten 1000 Zeichen zur Kontrolle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "093d4c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">Summarize today's top paper on Hugging Face daily papers.</span>                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•°â”€ LiteLLMModel - ollama_chat/glm-4.6:cloud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ•­â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mSummarize today's top paper on Hugging Face daily papers.\u001b[0m                                                       \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
       "\u001b[38;2;212;183;2mâ•°â”€\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama_chat/glm-4.6:cloud \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/anaconda3/envs/trans/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/anaconda3/envs/trans/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing parsed code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_hugging_face_top_daily_paper()</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"Top paper title: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting parsed code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_hugging_face_top_daily_paper\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTop paper title: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Top paper title: JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n",
       "  Code Intelligence\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Top paper title: JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n",
       "  Code Intelligence\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.13 seconds| Input tokens: 2,209 | Output tokens: 186]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.13 seconds| Input tokens: 2,209 | Output tokens: 186]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing parsed code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_paper_id_by_title(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Intelligence\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"Paper ID: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                 </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting parsed code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_paper_id_by_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mJanusCoder: Towards a Foundational Visual-Programmatic Interface for Code \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mIntelligence\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper ID: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Paper ID: 2510.23538\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Paper ID: 2510.23538\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.87 seconds| Input tokens: 4,591 | Output tokens: 374]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.87 seconds| Input tokens: 4,591 | Output tokens: 374]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep 3\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing parsed code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">download_path </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> download_paper_by_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"2510.23538\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"Paper downloaded to: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">download_path</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_content </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> read_pdf_file(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"paper.pdf\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                     </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(paper_content)</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting parsed code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mdownload_path\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdownload_paper_by_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m2510.23538\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper downloaded to: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdownload_path\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mread_pdf_file\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mpaper.pdf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Paper downloaded to: Downloaded JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code \n",
       "Intelligence as paper.pdf\n",
       "Preprint\n",
       "JANUSCODER: TOWARDS AFOUNDATIONALVISUAL-\n",
       "PROGRAMMATICINTERFACE FORCODEINTELLIGENCE\n",
       "Qiushi Sunâ™¡â™¢âˆ—\n",
       "Jingyang Gongâ™¡* Yang Liu* Qiaosheng Chen* Lei Liâ‹† Kai Chenâ™¢\n",
       "Qipeng Guoâ™¢á¼´BBen Kaoâ™¡ Fei Yuanâ™¢\n",
       "â™¡The University of Hong Kong â™¢Shanghai AI Laboratory Nanjing University\n",
       "â‹†Carnegie Mellon University á¼´BShanghai Innovation Institute\n",
       "qiushisun@connect.hku.hk,jingyang.gong@nyu.edu\n",
       "qschen@smail.nju.edu.cn,yliu20.nju@gmail.com,leili@cs.cmu.edu\n",
       "kao@cs.hku.hk,{chenkai,guoqipeng,yuanfei}@pjlab.org.cn\n",
       "ABSTRACT\n",
       "The scope of neural code intelligence is rapidly expanding beyond text-based source\n",
       "code to encompass the rich visual outputs that programs generate. This visual\n",
       "dimension is critical for advanced applications like flexible content generation and\n",
       "precise, program-driven editing of visualizations. However, progress has been im-\n",
       "peded by the scarcity of high-quality multi-modal code data, a bottleneck stemming\n",
       "from challenges in synthesis and quality assessment. To address these challenges,\n",
       "we make contributions from both a data and modeling perspective. We first in-\n",
       "troduce a complete synthesis toolkit that leverages reciprocal synergies between\n",
       "data modalities to efficiently produce a large-scale, high-quality corpus spanning\n",
       "from standard charts to complex interactive web UIs and code-driven animations.\n",
       "Leveraging this toolkit, we construct JANUSCODE-800K, the largest multimodal\n",
       "code corpus to date. This powers the training of our models, JANUSCODERand\n",
       "JANUSCODERV, which establish a visual-programmatic interface for generating\n",
       "code from textual instructions, visual inputs, or a combination of both. Our unified\n",
       "model is a departure from existing approaches that build specialized models for\n",
       "isolated tasks. Extensive experiments on both text-centric and vision-centric cod-\n",
       "ing tasks demonstrate the superior performance of the JANUSCODERseries, with\n",
       "our 7B to 14B scale models approaching or even exceeding the performance of\n",
       "commercial models. Furthermore, extensive analysis provides key insights into har-\n",
       "monizing programmatic logic with its visual expression. Our code and checkpoints\n",
       "are available athttps://github.com/InternLM/JanusCoder.\n",
       "1 INTRODUCTION\n",
       "The advent of Large Language Models (LLMs; Hurst et al., 2024; Anthropic AI, 2024) has signifi-\n",
       "cantly advanced the field of code intelligence (Sun et al., 2024a), revolutionizing tasks centered on\n",
       "textual source code. Building on this, the scope of code intelligence naturally expands beyond text to\n",
       "encompass the rich and diverse visual manifestations that programs generate (Gemini Team, 2025; Si\n",
       "et al., 2025), with the aspiration of bridging the perceptualâ€“symbolic gap. Establishing a generalist\n",
       "modeling interface that harmonizes codeâ€™s logic with its visual expression is therefore the next frontier.\n",
       "Such an interface would empower models to flexibly generate data visualizations (Galimzyanov et al.,\n",
       "2025; Ni et al., 2025) and interactive front-ends (Chen et al., 2025a;b), replicate or precisely edit\n",
       "visual artifacts from multimodal inputs (Yang et al., 2025b; Xia et al., 2025), and even build complex,\n",
       "code-driven animations (Ku et al., 2025) to elucidate a concept like â€œAttention Is All You Needâ€.\n",
       "Despite its promise, the connection between code and vision remains in its early stages. While\n",
       "recent models have shown success in handling unimodal symbolic representations (Xu et al., 2024b),\n",
       "extending this to multimodal scenarios presents far greater challenges. The first challenge lies at the\n",
       "modeling level. Current research predominantly focuses on program-aided understanding (Qiu et al.,\n",
       "2025; Chen et al., 2025c) and reasoning (SurÃ­s et al., 2023; Guo et al., 2025), while fine-grained\n",
       "âˆ—Equal Contribution.\n",
       "1\n",
       "arXiv:2510.23538v1  [cs.AI]  27 Oct 2025Preprint\n",
       "JanusCoder\n",
       "Takea screenshotanda piece of code of a web page,modify code according to the userâ€˜s instruction\n",
       "WebUIGeneration/Editing\n",
       "Replicate an interactive simple harmonic motion visualization based on the reference image\n",
       "DemonstrationGeneration\n",
       "Generate the Python code that can reproduce the picture based on the picture I provide.\n",
       "CharttoCode\n",
       "Build an interactive visualization of Gibbs free energy for a binary mixture\n",
       "Dynamic Visualizations\n",
       "build a web application for cellular automata\n",
       "VisualArtifacts\n",
       "Takea screenshotanda piece of code of a web page,modify code according to the userâ€˜s instruction\n",
       "AnimationGeneration\n",
       "&lt;divclass=\"column\"&gt;&lt;h3&gt;In&lt;/h3&gt;â€¦â€¦\n",
       "&lt;sectionclass=â€œvisu\"&gt;&lt;h4&gt;Evol&lt;/h4&gt;â€¦â€¦\n",
       "&lt;div&gt;&lt;label&gt;amplitudeâ€¦â€¦\n",
       "plt=Plot[G[x],{x,0,1},PlotStyle-&gt;Black]â€¦â€¦\n",
       "ax=fig.add_subplot()Surf=ax.plot_surfaceâ€¦â€¦\n",
       "frommanimimport*classAttPattern(Scene):â€¦â€¦\n",
       "Figure 1: JANUSCODERis a suite of models that establishes a unified visual-programmatic interface,\n",
       "advancing multimodal code intelligence. It supports diverse tasks by combining code with visual\n",
       "content generation, editing, and interpretation in a unified manner.\n",
       "perception (Liu et al., 2025) and generative capability remain significantly underdeveloped (Wang\n",
       "et al., 2025a). For the few well-explored scenarios (Wang et al., 2024; Yun et al., 2024), existing\n",
       "works often build specialized models for isolated targets (e.g., one for chart-to-code, another for\n",
       "WebUI-to-code), leading to models that can neither generalize across scenarios nor scale effectively.\n",
       "Second, and more fundamentally, progress is impeded by the scarcity of high-quality, diverse\n",
       "multimodal code data. The heterogeneity of content in existing corpora (Gui et al., 2025; Ni et al.,\n",
       "2025) presents a significant challenge, along with varying data richness across different programming\n",
       "languages (PLs), diverse styles of natural language (NL) instructions, and the vast array of visual\n",
       "outputs that code can produce. For instance, these visual outputs can range from static Matplotlib\n",
       "charts and interactive WebUIs to extended animations in the style of 3Blue1Brown 1. Creating a\n",
       "comprehensive corpus that covers this spectrum is a formidable task. It requires not only large-scale\n",
       "data collection and processing but also well-matched validation environments (e.g., computation /\n",
       "rendering engines), and rigorous quality control over the diverse visual contents.\n",
       "In this work, we are motivated to build a unified model to facilitate the development of multimodal\n",
       "code intelligence. Toward this goal, we make the following contributions:\n",
       "1. We develop and release a versatile data synthesis toolkit. This enables the automatic synthesis of\n",
       "multimodal code data across heterogeneous domains and PLs, including but not limited to charts,\n",
       "Web UIs, visual artifacts, and code-driven animations. By doing so, it significantly reduces the\n",
       "engineering efforts required for data curation in future research.\n",
       "2. Building on this data toolkit, we curate JANUSCODE-800K, the largest multimodal code intelli-\n",
       "gence corpus to date. Notably, our corpus includes large-scale animation and artifact data that have\n",
       "not been present in previous works.\n",
       "3. With the above data innovations and by fostering synergies across different modalities and tasks,\n",
       "we developed JANUSCODERand JANUSCODERV. As illustrated in Figure 1, these models constitute\n",
       "a unified interface designed to tackle a broad spectrum of visualâ€“programmatic tasks.\n",
       "4. We present a comprehensive evaluation, covering seven established and newly proposed bench-\n",
       "marks. Our models demonstrate superior performance improvements in both text-centric and vision-\n",
       "centric settings, approaching or even exceeding the performance of leading commercial models. This\n",
       "indicates that the JANUSCODERseries can serve as a strong open-source foundational model for\n",
       "future research and applications.\n",
       "1https://www.3blue1brown.com/\n",
       "2Preprint\n",
       "2 RELATEDWORKS\n",
       "Code Generation for Visual Interfaces.LLMs have been widely explored for text-centric code\n",
       "generation of visual interfaces, including data visualizations (Yang et al., 2024), web pages (Chen\n",
       "et al., 2025d), and interactive UIs (Chen et al., 2025a). Early efforts focused on Python libraries (e.g.,\n",
       "Matplotlib, Seaborn) for producing figures in scientific workflows (Zhang et al., 2024b; Sun et al.,\n",
       "2025b). Later work extended to chart generation and editing (Zhao et al., 2025a), and to mapping\n",
       "NL instructions into web-based artifacts (Zhang et al., 2025) or structured UI interactions (Cheng\n",
       "et al., 2024; Sun et al., 2024b). Overall, these approaches highlight the potential of LLMs to author\n",
       "executable visual content, though they remain constrained to text-driven inputs.\n",
       "Visually-Grounded Code Generation and Understanding.Another line of work emphasizes\n",
       "multimodal inputs (vision-centric), where models interpret visual information to produce or reason\n",
       "about symbolic code (Hu et al., 2024; Jiang et al., 2025). Representative efforts include chart\n",
       "understanding, which evaluates the extraction of structured knowledge from plots (Masry et al.,\n",
       "2022; Zhang et al., 2024a), and chart-to-code generation, which requires reproducing scientific plots\n",
       "from images with captions or instructions (Zhao et al., 2025b; Xia et al., 2025; Wu et al., 2025).\n",
       "Beyond charts, studies extend to theorem visualization (Ku et al., 2025), multimodal algorithmic\n",
       "problem solving (Li et al., 2024), and structured vector graphics such as SVGs (Yang et al., 2025c;\n",
       "Nishina &amp; Matsui, 2024). While these works demonstrate progress, they largely target isolated\n",
       "domains and modalities. In contrast, we move beyond these constraints by unifying diverse domains\n",
       "and modalities across charts, web UIs, animations, symbolic computation, and more, taking a leap\n",
       "forward in advancing multimodal code intelligence.\n",
       "3 METHOD\n",
       "To empower models for multimodal code intelligence, we propose a versatile data toolkit that\n",
       "incorporates model interactions (Sun et al., 2023b) and compiler feedback to tackle multifaceted\n",
       "demands. In contrast to prior data approaches, which often suffer from a lack of instruction diversity,\n",
       "scarcity in specialized domains, and insufficient validation for visual-code alignment, our pipeline\n",
       "establishes a principled workflow. As shown in Figure 2: (1) Data Sourcing, where raw assets\n",
       "are collected and categorized; (2) Data Synthesis &amp; Curation, where new instruction-code pairs is\n",
       "generated and refined through a multi-strategy engine; and (3) Quality Control, which ensures data\n",
       "fidelity through automated validation and LLM/VLM judging.\n",
       "CollectionandCategorization\n",
       "PublicRepos\n",
       "Algorithms\n",
       "Artifacts\n",
       "CodeArchive\n",
       "WebPages\n",
       "Matplotlib, WebUI, General Artifacts\n",
       "R,MATLAB,SVG\n",
       "Scientific Demonstrations\n",
       "Animation,Mathematica\n",
       "NL + Code(a) Guided Evolution\n",
       "Code\n",
       "NL + Code\n",
       "NL + Code\n",
       "(b) Reverse Instruction\n",
       "(c) Re-contextualization\n",
       "(d) Bidirectional Translation\n",
       "Evolve\n",
       "Generate\n",
       "Recontext\n",
       "Translate\n",
       "Validate\n",
       "NLâ€™ + Codeâ€™\n",
       "NL + Codeâ€™\n",
       "NLâ€™ + Codeâ€™\n",
       "Curation\n",
       "NLâ€™ + Codeâ€™\n",
       "Refine &amp; Retry\n",
       "Reward5/5\n",
       "InitialDataset\n",
       " Scoring and Filtering\n",
       "Final DatasetJanusCode-800K\n",
       "LLM Judge\n",
       "VLM JudgeNL  Code  Img\n",
       "NL  Code\n",
       "â€¦â€¦\n",
       "(a) Stage 1(b) Stage 2(c) Stage 3\n",
       "Reward2/5\n",
       "â€¦â€¦\n",
       "Animations\n",
       "Concept\n",
       "RewardModeling\n",
       "Figure 2: An overview of our toolkit for curating JANUSCODE-800K, which integrates heterogeneous\n",
       "data sourcing, multi-strategy synthesis and curation, and LLM/VLM-based reward modeling after\n",
       "execution checks.\n",
       "3.1 DATACOLLECTION\n",
       "Our pipeline begins by aggregating raw data from a vast and heterogeneous sources. These include\n",
       "large-scale public datasets (e.g., StackV2; Lozhkov et al., 2024), extensive web corpora (e.g., Web-\n",
       "3\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Paper downloaded to: Downloaded JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code \n",
       "Intelligence as paper.pdf\n",
       "Preprint\n",
       "JANUSCODER: TOWARDS AFOUNDATIONALVISUAL-\n",
       "PROGRAMMATICINTERFACE FORCODEINTELLIGENCE\n",
       "Qiushi Sunâ™¡â™¢âˆ—\n",
       "Jingyang Gongâ™¡* Yang Liu* Qiaosheng Chen* Lei Liâ‹† Kai Chenâ™¢\n",
       "Qipeng Guoâ™¢á¼´BBen Kaoâ™¡ Fei Yuanâ™¢\n",
       "â™¡The University of Hong Kong â™¢Shanghai AI Laboratory Nanjing University\n",
       "â‹†Carnegie Mellon University á¼´BShanghai Innovation Institute\n",
       "qiushisun@connect.hku.hk,jingyang.gong@nyu.edu\n",
       "qschen@smail.nju.edu.cn,yliu20.nju@gmail.com,leili@cs.cmu.edu\n",
       "kao@cs.hku.hk,{chenkai,guoqipeng,yuanfei}@pjlab.org.cn\n",
       "ABSTRACT\n",
       "The scope of neural code intelligence is rapidly expanding beyond text-based source\n",
       "code to encompass the rich visual outputs that programs generate. This visual\n",
       "dimension is critical for advanced applications like flexible content generation and\n",
       "precise, program-driven editing of visualizations. However, progress has been im-\n",
       "peded by the scarcity of high-quality multi-modal code data, a bottleneck stemming\n",
       "from challenges in synthesis and quality assessment. To address these challenges,\n",
       "we make contributions from both a data and modeling perspective. We first in-\n",
       "troduce a complete synthesis toolkit that leverages reciprocal synergies between\n",
       "data modalities to efficiently produce a large-scale, high-quality corpus spanning\n",
       "from standard charts to complex interactive web UIs and code-driven animations.\n",
       "Leveraging this toolkit, we construct JANUSCODE-800K, the largest multimodal\n",
       "code corpus to date. This powers the training of our models, JANUSCODERand\n",
       "JANUSCODERV, which establish a visual-programmatic interface for generating\n",
       "code from textual instructions, visual inputs, or a combination of both. Our unified\n",
       "model is a departure from existing approaches that build specialized models for\n",
       "isolated tasks. Extensive experiments on both text-centric and vision-centric cod-\n",
       "ing tasks demonstrate the superior performance of the JANUSCODERseries, with\n",
       "our 7B to 14B scale models approaching or even exceeding the performance of\n",
       "commercial models. Furthermore, extensive analysis provides key insights into har-\n",
       "monizing programmatic logic with its visual expression. Our code and checkpoints\n",
       "are available athttps://github.com/InternLM/JanusCoder.\n",
       "1 INTRODUCTION\n",
       "The advent of Large Language Models (LLMs; Hurst et al., 2024; Anthropic AI, 2024) has signifi-\n",
       "cantly advanced the field of code intelligence (Sun et al., 2024a), revolutionizing tasks centered on\n",
       "textual source code. Building on this, the scope of code intelligence naturally expands beyond text to\n",
       "encompass the rich and diverse visual manifestations that programs generate (Gemini Team, 2025; Si\n",
       "et al., 2025), with the aspiration of bridging the perceptualâ€“symbolic gap. Establishing a generalist\n",
       "modeling interface that harmonizes codeâ€™s logic with its visual expression is therefore the next frontier.\n",
       "Such an interface would empower models to flexibly generate data visualizations (Galimzyanov et al.,\n",
       "2025; Ni et al., 2025) and interactive front-ends (Chen et al., 2025a;b), replicate or precisely edit\n",
       "visual artifacts from multimodal inputs (Yang et al., 2025b; Xia et al., 2025), and even build complex,\n",
       "code-driven animations (Ku et al., 2025) to elucidate a concept like â€œAttention Is All You Needâ€.\n",
       "Despite its promise, the connection between code and vision remains in its early stages. While\n",
       "recent models have shown success in handling unimodal symbolic representations (Xu et al., 2024b),\n",
       "extending this to multimodal scenarios presents far greater challenges. The first challenge lies at the\n",
       "modeling level. Current research predominantly focuses on program-aided understanding (Qiu et al.,\n",
       "2025; Chen et al., 2025c) and reasoning (SurÃ­s et al., 2023; Guo et al., 2025), while fine-grained\n",
       "âˆ—Equal Contribution.\n",
       "1\n",
       "arXiv:2510.23538v1  [cs.AI]  27 Oct 2025Preprint\n",
       "JanusCoder\n",
       "Takea screenshotanda piece of code of a web page,modify code according to the userâ€˜s instruction\n",
       "WebUIGeneration/Editing\n",
       "Replicate an interactive simple harmonic motion visualization based on the reference image\n",
       "DemonstrationGeneration\n",
       "Generate the Python code that can reproduce the picture based on the picture I provide.\n",
       "CharttoCode\n",
       "Build an interactive visualization of Gibbs free energy for a binary mixture\n",
       "Dynamic Visualizations\n",
       "build a web application for cellular automata\n",
       "VisualArtifacts\n",
       "Takea screenshotanda piece of code of a web page,modify code according to the userâ€˜s instruction\n",
       "AnimationGeneration\n",
       "<divclass=\"column\"><h3>In</h3>â€¦â€¦\n",
       "<sectionclass=â€œvisu\"><h4>Evol</h4>â€¦â€¦\n",
       "<div><label>amplitudeâ€¦â€¦\n",
       "plt=Plot[G[x],{x,0,1},PlotStyle->Black]â€¦â€¦\n",
       "ax=fig.add_subplot()Surf=ax.plot_surfaceâ€¦â€¦\n",
       "frommanimimport*classAttPattern(Scene):â€¦â€¦\n",
       "Figure 1: JANUSCODERis a suite of models that establishes a unified visual-programmatic interface,\n",
       "advancing multimodal code intelligence. It supports diverse tasks by combining code with visual\n",
       "content generation, editing, and interpretation in a unified manner.\n",
       "perception (Liu et al., 2025) and generative capability remain significantly underdeveloped (Wang\n",
       "et al., 2025a). For the few well-explored scenarios (Wang et al., 2024; Yun et al., 2024), existing\n",
       "works often build specialized models for isolated targets (e.g., one for chart-to-code, another for\n",
       "WebUI-to-code), leading to models that can neither generalize across scenarios nor scale effectively.\n",
       "Second, and more fundamentally, progress is impeded by the scarcity of high-quality, diverse\n",
       "multimodal code data. The heterogeneity of content in existing corpora (Gui et al., 2025; Ni et al.,\n",
       "2025) presents a significant challenge, along with varying data richness across different programming\n",
       "languages (PLs), diverse styles of natural language (NL) instructions, and the vast array of visual\n",
       "outputs that code can produce. For instance, these visual outputs can range from static Matplotlib\n",
       "charts and interactive WebUIs to extended animations in the style of 3Blue1Brown 1. Creating a\n",
       "comprehensive corpus that covers this spectrum is a formidable task. It requires not only large-scale\n",
       "data collection and processing but also well-matched validation environments (e.g., computation /\n",
       "rendering engines), and rigorous quality control over the diverse visual contents.\n",
       "In this work, we are motivated to build a unified model to facilitate the development of multimodal\n",
       "code intelligence. Toward this goal, we make the following contributions:\n",
       "1. We develop and release a versatile data synthesis toolkit. This enables the automatic synthesis of\n",
       "multimodal code data across heterogeneous domains and PLs, including but not limited to charts,\n",
       "Web UIs, visual artifacts, and code-driven animations. By doing so, it significantly reduces the\n",
       "engineering efforts required for data curation in future research.\n",
       "2. Building on this data toolkit, we curate JANUSCODE-800K, the largest multimodal code intelli-\n",
       "gence corpus to date. Notably, our corpus includes large-scale animation and artifact data that have\n",
       "not been present in previous works.\n",
       "3. With the above data innovations and by fostering synergies across different modalities and tasks,\n",
       "we developed JANUSCODERand JANUSCODERV. As illustrated in Figure 1, these models constitute\n",
       "a unified interface designed to tackle a broad spectrum of visualâ€“programmatic tasks.\n",
       "4. We present a comprehensive evaluation, covering seven established and newly proposed bench-\n",
       "marks. Our models demonstrate superior performance improvements in both text-centric and vision-\n",
       "centric settings, approaching or even exceeding the performance of leading commercial models. This\n",
       "indicates that the JANUSCODERseries can serve as a strong open-source foundational model for\n",
       "future research and applications.\n",
       "1https://www.3blue1brown.com/\n",
       "2Preprint\n",
       "2 RELATEDWORKS\n",
       "Code Generation for Visual Interfaces.LLMs have been widely explored for text-centric code\n",
       "generation of visual interfaces, including data visualizations (Yang et al., 2024), web pages (Chen\n",
       "et al., 2025d), and interactive UIs (Chen et al., 2025a). Early efforts focused on Python libraries (e.g.,\n",
       "Matplotlib, Seaborn) for producing figures in scientific workflows (Zhang et al., 2024b; Sun et al.,\n",
       "2025b). Later work extended to chart generation and editing (Zhao et al., 2025a), and to mapping\n",
       "NL instructions into web-based artifacts (Zhang et al., 2025) or structured UI interactions (Cheng\n",
       "et al., 2024; Sun et al., 2024b). Overall, these approaches highlight the potential of LLMs to author\n",
       "executable visual content, though they remain constrained to text-driven inputs.\n",
       "Visually-Grounded Code Generation and Understanding.Another line of work emphasizes\n",
       "multimodal inputs (vision-centric), where models interpret visual information to produce or reason\n",
       "about symbolic code (Hu et al., 2024; Jiang et al., 2025). Representative efforts include chart\n",
       "understanding, which evaluates the extraction of structured knowledge from plots (Masry et al.,\n",
       "2022; Zhang et al., 2024a), and chart-to-code generation, which requires reproducing scientific plots\n",
       "from images with captions or instructions (Zhao et al., 2025b; Xia et al., 2025; Wu et al., 2025).\n",
       "Beyond charts, studies extend to theorem visualization (Ku et al., 2025), multimodal algorithmic\n",
       "problem solving (Li et al., 2024), and structured vector graphics such as SVGs (Yang et al., 2025c;\n",
       "Nishina & Matsui, 2024). While these works demonstrate progress, they largely target isolated\n",
       "domains and modalities. In contrast, we move beyond these constraints by unifying diverse domains\n",
       "and modalities across charts, web UIs, animations, symbolic computation, and more, taking a leap\n",
       "forward in advancing multimodal code intelligence.\n",
       "3 METHOD\n",
       "To empower models for multimodal code intelligence, we propose a versatile data toolkit that\n",
       "incorporates model interactions (Sun et al., 2023b) and compiler feedback to tackle multifaceted\n",
       "demands. In contrast to prior data approaches, which often suffer from a lack of instruction diversity,\n",
       "scarcity in specialized domains, and insufficient validation for visual-code alignment, our pipeline\n",
       "establishes a principled workflow. As shown in Figure 2: (1) Data Sourcing, where raw assets\n",
       "are collected and categorized; (2) Data Synthesis & Curation, where new instruction-code pairs is\n",
       "generated and refined through a multi-strategy engine; and (3) Quality Control, which ensures data\n",
       "fidelity through automated validation and LLM/VLM judging.\n",
       "CollectionandCategorization\n",
       "PublicRepos\n",
       "Algorithms\n",
       "Artifacts\n",
       "CodeArchive\n",
       "WebPages\n",
       "Matplotlib, WebUI, General Artifacts\n",
       "R,MATLAB,SVG\n",
       "Scientific Demonstrations\n",
       "Animation,Mathematica\n",
       "NL + Code(a) Guided Evolution\n",
       "Code\n",
       "NL + Code\n",
       "NL + Code\n",
       "(b) Reverse Instruction\n",
       "(c) Re-contextualization\n",
       "(d) Bidirectional Translation\n",
       "Evolve\n",
       "Generate\n",
       "Recontext\n",
       "Translate\n",
       "Validate\n",
       "NLâ€™ + Codeâ€™\n",
       "NL + Codeâ€™\n",
       "NLâ€™ + Codeâ€™\n",
       "Curation\n",
       "NLâ€™ + Codeâ€™\n",
       "Refine & Retry\n",
       "Reward5/5\n",
       "InitialDataset\n",
       " Scoring and Filtering\n",
       "Final DatasetJanusCode-800K\n",
       "LLM Judge\n",
       "VLM JudgeNL  Code  Img\n",
       "NL  Code\n",
       "â€¦â€¦\n",
       "(a) Stage 1(b) Stage 2(c) Stage 3\n",
       "Reward2/5\n",
       "â€¦â€¦\n",
       "Animations\n",
       "Concept\n",
       "RewardModeling\n",
       "Figure 2: An overview of our toolkit for curating JANUSCODE-800K, which integrates heterogeneous\n",
       "data sourcing, multi-strategy synthesis and curation, and LLM/VLM-based reward modeling after\n",
       "execution checks.\n",
       "3.1 DATACOLLECTION\n",
       "Our pipeline begins by aggregating raw data from a vast and heterogeneous sources. These include\n",
       "large-scale public datasets (e.g., StackV2; Lozhkov et al., 2024), extensive web corpora (e.g., Web-\n",
       "3\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 7.20 seconds| Input tokens: 7,259 | Output tokens: 698]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 7.20 seconds| Input tokens: 7,259 | Output tokens: 698]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step 4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep 4\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> â”€ <span style=\"font-weight: bold\">Executing parsed code:</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"\"\"Today's top Hugging Face paper is \"JanusCoder: Towards a Foundational Visual-Programmatic </span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Interface for Code Intelligence.\"</span><span style=\"background-color: #272822\">                                                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Key Summary:</span><span style=\"background-color: #272822\">                                                                                                   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">JanusCoder introduces a unified model that bridges the gap between code logic and visual expression, enabling </span><span style=\"background-color: #272822\"> </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">multimodal code intelligence. The research addresses two major challenges: the lack of unified modeling </span><span style=\"background-color: #272822\">       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">interfaces and scarcity of high-quality multimodal code data.</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Main Contributions:</span><span style=\"background-color: #272822\">                                                                                            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">1. **Data Synthesis Toolkit**: A versatile toolkit for automatically generating multimodal code data across </span><span style=\"background-color: #272822\">   </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">diverse domains including charts, web UIs, visual artifacts, and code-driven animations.</span><span style=\"background-color: #272822\">                       </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">2. **JANUSCODE-800K Dataset**: The largest multimodal code corpus to date, featuring unprecedented scale and </span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">diversity including animation and artifact data not present in previous works.</span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">3. **JanusCoder Models**: Two unified models (JANUSCODER and JANUSCODERV) that establish a visual-programmatic </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">interface supporting diverse tasks like chart generation, web UI creation, animation production, and code </span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">editing from visual inputs.</span><span style=\"background-color: #272822\">                                                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">4. **Superior Performance**: The 7B to 14B scale models approach or exceed commercial model performance across </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">both text-centric and vision-centric coding tasks.</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">The unified approach represents a significant departure from existing specialized models, enabling </span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">generalization across different modalities and scenarios. The work advances the field by harmonizing </span><span style=\"background-color: #272822\">          </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">programmatic logic with visual expression, opening new possibilities for flexible content generation and </span><span style=\"background-color: #272822\">      </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">precise, program-driven editing of visualizations.</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Code and checkpoints are available at: https://github.com/InternLM/JanusCoder\"\"\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                              </span>  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " â”€ \u001b[1mExecuting parsed code:\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\"\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mToday\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34ms top Hugging Face paper is \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mJanusCoder: Towards a Foundational Visual-Programmatic \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mInterface for Code Intelligence.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mKey Summary:\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mJanusCoder introduces a unified model that bridges the gap between code logic and visual expression, enabling \u001b[0m\u001b[48;2;39;40;34m \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mmultimodal code intelligence. The research addresses two major challenges: the lack of unified modeling \u001b[0m\u001b[48;2;39;40;34m       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34minterfaces and scarcity of high-quality multimodal code data.\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mMain Contributions:\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m1. **Data Synthesis Toolkit**: A versatile toolkit for automatically generating multimodal code data across \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mdiverse domains including charts, web UIs, visual artifacts, and code-driven animations.\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m2. **JANUSCODE-800K Dataset**: The largest multimodal code corpus to date, featuring unprecedented scale and \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mdiversity including animation and artifact data not present in previous works.\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m3. **JanusCoder Models**: Two unified models (JANUSCODER and JANUSCODERV) that establish a visual-programmatic \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34minterface supporting diverse tasks like chart generation, web UI creation, animation production, and code \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mediting from visual inputs.\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m4. **Superior Performance**: The 7B to 14B scale models approach or exceed commercial model performance across \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mboth text-centric and vision-centric coding tasks.\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mThe unified approach represents a significant departure from existing specialized models, enabling \u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mgeneralization across different modalities and scenarios. The work advances the field by harmonizing \u001b[0m\u001b[48;2;39;40;34m          \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mprogrammatic logic with visual expression, opening new possibilities for flexible content generation and \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mprecise, program-driven editing of visualizations.\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mCode and checkpoints are available at: https://github.com/InternLM/JanusCoder\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\"\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m  \n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Today's top Hugging Face paper is \"JanusCoder: Towards a Foundational Visual-Programmatic Interface </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">for Code Intelligence.\"</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Key Summary:</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">JanusCoder introduces a unified model that bridges the gap between code logic and visual expression, enabling </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">multimodal code intelligence. The research addresses two major challenges: the lack of unified modeling interfaces </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">and scarcity of high-quality multimodal code data.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Main Contributions:</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. **Data Synthesis Toolkit**: A versatile toolkit for automatically generating multimodal code data across diverse</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">domains including charts, web UIs, visual artifacts, and code-driven animations.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. **JANUSCODE-800K Dataset**: The largest multimodal code corpus to date, featuring unprecedented scale and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">diversity including animation and artifact data not present in previous works.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. **JanusCoder Models**: Two unified models (JANUSCODER and JANUSCODERV) that establish a visual-programmatic </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">interface supporting diverse tasks like chart generation, web UI creation, animation production, and code editing </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">from visual inputs.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. **Superior Performance**: The 7B to 14B scale models approach or exceed commercial model performance across both</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">text-centric and vision-centric coding tasks.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">The unified approach represents a significant departure from existing specialized models, enabling generalization </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">across different modalities and scenarios. The work advances the field by harmonizing programmatic logic with </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">visual expression, opening new possibilities for flexible content generation and precise, program-driven editing of</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">visualizations.</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Code and checkpoints are available at: https://github.com/InternLM/JanusCoder</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Today's top Hugging Face paper is \"JanusCoder: Towards a Foundational Visual-Programmatic Interface \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfor Code Intelligence.\"\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mKey Summary:\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mJanusCoder introduces a unified model that bridges the gap between code logic and visual expression, enabling \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmultimodal code intelligence. The research addresses two major challenges: the lack of unified modeling interfaces \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mand scarcity of high-quality multimodal code data.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mMain Contributions:\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m1. **Data Synthesis Toolkit**: A versatile toolkit for automatically generating multimodal code data across diverse\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mdomains including charts, web UIs, visual artifacts, and code-driven animations.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m2. **JANUSCODE-800K Dataset**: The largest multimodal code corpus to date, featuring unprecedented scale and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mdiversity including animation and artifact data not present in previous works.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m3. **JanusCoder Models**: Two unified models (JANUSCODER and JANUSCODERV) that establish a visual-programmatic \u001b[0m\n",
       "\u001b[1;38;2;212;183;2minterface supporting diverse tasks like chart generation, web UI creation, animation production, and code editing \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfrom visual inputs.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m4. **Superior Performance**: The 7B to 14B scale models approach or exceed commercial model performance across both\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtext-centric and vision-centric coding tasks.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mThe unified approach represents a significant departure from existing specialized models, enabling generalization \u001b[0m\n",
       "\u001b[1;38;2;212;183;2macross different modalities and scenarios. The work advances the field by harmonizing programmatic logic with \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mvisual expression, opening new possibilities for flexible content generation and precise, program-driven editing of\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mvisualizations.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mCode and checkpoints are available at: https://github.com/InternLM/JanusCoder\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 3.90 seconds| Input tokens: 13,067 | Output tokens: 1,077]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 3.90 seconds| Input tokens: 13,067 | Output tokens: 1,077]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Today\\'s top Hugging Face paper is \"JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence.\"\\n\\nKey Summary:\\nJanusCoder introduces a unified model that bridges the gap between code logic and visual expression, enabling multimodal code intelligence. The research addresses two major challenges: the lack of unified modeling interfaces and scarcity of high-quality multimodal code data.\\n\\nMain Contributions:\\n1. **Data Synthesis Toolkit**: A versatile toolkit for automatically generating multimodal code data across diverse domains including charts, web UIs, visual artifacts, and code-driven animations.\\n\\n2. **JANUSCODE-800K Dataset**: The largest multimodal code corpus to date, featuring unprecedented scale and diversity including animation and artifact data not present in previous works.\\n\\n3. **JanusCoder Models**: Two unified models (JANUSCODER and JANUSCODERV) that establish a visual-programmatic interface supporting diverse tasks like chart generation, web UI creation, animation production, and code editing from visual inputs.\\n\\n4. **Superior Performance**: The 7B to 14B scale models approach or exceed commercial model performance across both text-centric and vision-centric coding tasks.\\n\\nThe unified approach represents a significant departure from existing specialized models, enabling generalization across different modalities and scenarios. The work advances the field by harmonizing programmatic logic with visual expression, opening new possibilities for flexible content generation and precise, program-driven editing of visualizations.\\n\\nCode and checkpoints are available at: https://github.com/InternLM/JanusCoder'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smolagents import CodeAgent, LiteLLMModel\n",
    "\n",
    "model = LiteLLMModel(model_id=\"ollama_chat/glm-4.6:cloud\")\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[\n",
    "        get_hugging_face_top_daily_paper,\n",
    "        get_paper_id_by_title,\n",
    "        download_paper_by_id,\n",
    "        read_pdf_file\n",
    "    ],\n",
    "    model=model,\n",
    "    stream_outputs=True\n",
    ")\n",
    "\n",
    "agent.run(\"Summarize today's top paper on Hugging Face daily papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6accd99",
   "metadata": {},
   "outputs": [
    {
     "ename": "DocstringParsingException",
     "evalue": "Cannot generate JSON schema for download_paper_by_id because the docstring has no description for the argument 'paper_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDocstringParsingException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo paper ID found.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# 3ï¸âƒ£ Paper herunterladen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;129;43m@tool\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mdownload_paper_by_id\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[33;43;03m    Downloads the arXiv paper by ID and saves it as 'paper_<id>.pdf'.\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[33;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/trans/lib/python3.12/site-packages/smolagents/tools.py:1071\u001b[39m, in \u001b[36mtool\u001b[39m\u001b[34m(tool_function)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtool\u001b[39m(tool_function: Callable) -> Tool:\n\u001b[32m   1062\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[33;03m    Convert a function into an instance of a dynamically created Tool subclass.\u001b[39;00m\n\u001b[32m   1064\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1069\u001b[39m \u001b[33;03m            and an 'Args:' part where each argument is described.\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m     tool_json_schema = \u001b[43mget_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_function\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tool_json_schema:\n\u001b[32m   1073\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tool_json_schema[\u001b[33m\"\u001b[39m\u001b[33mparameters\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m]) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/trans/lib/python3.12/site-packages/smolagents/_function_type_hints_utils.py:218\u001b[39m, in \u001b[36mget_json_schema\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg, schema \u001b[38;5;129;01min\u001b[39;00m json_schema[\u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m].items():\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_descriptions:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m DocstringParsingException(\n\u001b[32m    219\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot generate JSON schema for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because the docstring has no description for the argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    220\u001b[39m         )\n\u001b[32m    221\u001b[39m     desc = param_descriptions[arg]\n\u001b[32m    222\u001b[39m     enum_choices = re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m(choices:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*(.*?)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*$\u001b[39m\u001b[33m\"\u001b[39m, desc, flags=re.IGNORECASE)\n",
      "\u001b[31mDocstringParsingException\u001b[39m: Cannot generate JSON schema for download_paper_by_id because the docstring has no description for the argument 'paper_id'"
     ]
    }
   ],
   "source": [
    "from smolagents import tool, CodeAgent, LiteLLMModel\n",
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "from huggingface_hub import HfApi\n",
    "import arxiv\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# 1ï¸âƒ£ Mehrere Paper holen\n",
    "@tool\n",
    "def get_top_three_papers() -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of the top 3 daily papers (titles) from Hugging Face.\n",
    "    \"\"\"\n",
    "    url = \"https://huggingface.co/papers\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    containers = soup.find_all('div', class_='SVELTE_HYDRATER contents')\n",
    "    titles = []\n",
    "\n",
    "    for container in containers:\n",
    "        data_props = container.get('data-props', '')\n",
    "        if not data_props:\n",
    "            continue\n",
    "        try:\n",
    "            data_json = json.loads(data_props.replace('&quot;', '\"'))\n",
    "            if 'dailyPapers' in data_json:\n",
    "                for paper in data_json['dailyPapers'][:3]:\n",
    "                    titles.append(paper['title'])\n",
    "                break\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    return titles or [\"No papers found.\"]\n",
    "\n",
    "\n",
    "# 2ï¸âƒ£ ID finden\n",
    "@tool\n",
    "def get_paper_id_by_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the arXiv paper ID by its title.\n",
    "    Args:\n",
    "        title: The paper title for which to get the ID.\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    papers = api.list_papers(query=title)\n",
    "    if papers:\n",
    "        paper = next(iter(papers))\n",
    "        return paper.id\n",
    "    else:\n",
    "        return \"No paper ID found.\"\n",
    "\n",
    "\n",
    "# 3ï¸âƒ£ Paper herunterladen\n",
    "@tool\n",
    "def download_paper_by_id(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads the arXiv paper by ID and saves it as 'paper_<id>.pdf'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        paper = next(client.results(search))\n",
    "        filename = f\"paper_{paper_id}.pdf\"\n",
    "        paper.download_pdf(filename=filename)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {e}\"\n",
    "\n",
    "\n",
    "# 4ï¸âƒ£ PDF lesen\n",
    "@tool\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the first 3 pages of a PDF and returns text.\n",
    "    Args:\n",
    "        file_path: Path to the PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages[:3]:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading {file_path}: {e}\"\n",
    "\n",
    "\n",
    "# 5ï¸âƒ£ Agent definieren\n",
    "model = LiteLLMModel(model_id=\"ollama_chat/glm-4.6:cloud\")\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[get_top_three_papers, get_paper_id_by_title, download_paper_by_id, read_pdf_file],\n",
    "    model=model,\n",
    "    stream_outputs=True\n",
    ")\n",
    "\n",
    "# 6ï¸âƒ£ Ziel definieren\n",
    "agent.run(\"\"\"\n",
    "Fetch the top 3 papers from Hugging Face daily papers.\n",
    "For each paper:\n",
    "- get its arXiv ID,\n",
    "- download it,\n",
    "- read the first pages,\n",
    "- and summarize it.\n",
    "Then create a short consolidated summary comparing the three papers.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
